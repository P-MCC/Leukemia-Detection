{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\nfrom keras.losses import sparse_categorical_crossentropy\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import KFold\nimport numpy as np\nimport json\nimport math\nimport os\nimport cv2\nfrom PIL import Image\nimport numpy as np\nfrom keras import layers\nfrom keras.applications import ResNet50,MobileNet, DenseNet201, InceptionV3, NASNetLarge, InceptionResNetV2, NASNetMobile,VGG16,Xception\nfrom keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau, TensorBoard,CSVLogger\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import cohen_kappa_score, accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nimport scipy\nfrom tqdm import tqdm\nimport tensorflow as tf\nfrom keras import backend as K\nimport gc\nfrom functools import partial\nfrom sklearn import metrics\nfrom collections import Counter\nimport json\nimport itertools\nfrom sklearn.metrics import f1_score\n#confusion matrix\nfrom sklearn.metrics import confusion_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model configuration\nbatch_size = 32\nno_epochs = 10\noptimizer = Adam()\nverbosity = 1\nnum_folds = 10","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Transfer 'jpg' images to an array IMG\ndef Dataset_loader(DIR, RESIZE, sigmaX=10):\n    IMG = []\n    read = lambda imname: np.asarray(Image.open(imname).convert(\"RGB\"))\n    for IMAGE_NAME in tqdm(os.listdir(DIR)):\n        PATH = os.path.join(DIR,IMAGE_NAME)\n        _, ftype = os.path.splitext(PATH)\n        if ftype == \".jpg\":\n            img = read(PATH)\n\n            img = cv2.resize(img, (RESIZE,RESIZE))\n\n            IMG.append(np.array(img))\n    return IMG\n\ndata_path = \"/kaggle/input/x3data/x3\"\n\nbenign_train = np.array(Dataset_loader(data_path+'/train/Healthy',224))\nmalign_train = np.array(Dataset_loader(data_path+'/train/Infected',224))\nbenign_test = np.array(Dataset_loader(data_path+'/test/Healthy',224))\nmalign_test = np.array(Dataset_loader(data_path+'/test/Infected',224))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create labels\nbenign_train_label = np.zeros(len(benign_train))\nmalign_train_label = np.ones(len(malign_train))\nbenign_test_label = np.zeros(len(benign_test))\nmalign_test_label = np.ones(len(malign_test))\n\n# Merge data \nX_train = np.concatenate((benign_train, malign_train), axis = 0)\nY_train = np.concatenate((benign_train_label, malign_train_label), axis = 0)\nX_test = np.concatenate((benign_test, malign_test), axis = 0)\nY_test = np.concatenate((benign_test_label, malign_test_label), axis = 0)\n\n# Shuffle train data\ns = np.arange(X_train.shape[0])\nnp.random.shuffle(s)\nX_train = X_train[s]\nY_train = Y_train[s]\n\n# Shuffle test data\ns = np.arange(X_test.shape[0])\nnp.random.shuffle(s)\nX_test = X_test[s]\nY_test = Y_test[s]\n\n# To categorical\nY_train = to_categorical(Y_train, num_classes= 2)\nY_test = to_categorical(Y_test, num_classes= 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define per-fold score containers\nacc_per_fold = []\nloss_per_fold = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge inputs and targets\ninputs = np.concatenate((X_train, X_test), axis=0)\ntargets = np.concatenate((Y_train, Y_test), axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(backbone, lr=1e-4):\n    model = Sequential()\n    model.add(backbone)\n    model.add(layers.GlobalAveragePooling2D())\n    model.add(layers.Dropout(0.5))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Dense(2, activation='softmax'))\n    \n    \n    model.compile(\n        loss='binary_crossentropy',\n        optimizer=Adam(lr=lr),\n        metrics=['accuracy']\n    )\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Learning Rate Reducer\nlearn_control = ReduceLROnPlateau(monitor='val_accuracy', patience=10,\n                                  verbose=1,factor=0.2, min_lr=1e-7)\n\n# Checkpoint\nfilepath=\"VGG16_x5.weights.best.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\ncallbacks_list = [checkpoint]\ncsv_logger = CSVLogger(\"model_history_log.csv\", append=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the K-fold Cross Validator\nkfold = KFold(n_splits=num_folds, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=55)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# K-fold Cross Validation model evaluation\nloss_function = sparse_categorical_crossentropy\nfold_no = 1\nf1_scores = []\nfor train, test in kfold.split(inputs, targets):\n\n    K.clear_session()\n    gc.collect()\n\n    vgg = ResNet50(\n        weights='imagenet',\n        include_top=False,\n        input_shape=(224,224,3)\n    )\n    model = build_model(vgg ,lr = 1e-4)\n \n\n\n  # Generate a print\n    print('------------------------------------------------------------------------')\n    print(f'Training for fold {fold_no} ...')\n\n  # Fit data to model\n    history = model.fit(\n                inputs[train], targets[train],\n                \n                validation_data=(inputs[test], targets[test]),\n                batch_size=batch_size,\n                epochs=no_epochs,\n                verbose=verbosity,\n                callbacks=[learn_control, checkpoint, csv_logger]\n                )\n\n  # Generate generalization metrics\n    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n    acc_per_fold.append(scores[1] * 100)\n    loss_per_fold.append(scores[0])\n    \n     # Calculate F1 score\n    y_pred = model.predict(inputs[test])\n    y_pred_classes = np.argmax(y_pred, axis=1)\n    fold_f1_score = f1_score(np.argmax(targets[test], axis=1), y_pred_classes)\n    f1_scores.append(fold_f1_score)\n    \n    Y_val_pred = model.predict(X_test)\n  \n    Y_pred = model.predict(X_test)    \n    \n    cm = confusion_matrix(np.argmax(Y_test, axis=1), np.argmax(Y_pred, axis=1))\n\n    cm_plot_label =['healthy', 'infected']\n    plot_confusion_matrix(cm, cm_plot_label, title ='Confusion Metrix for ALL')\n\n  # Increase fold number\n    fold_no = fold_no + 1\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f1_scores)\n\n# Calculate average F1 score\naverage_f1_score = np.mean(f1_scores)\n\n# Print average F1 score\nprint(\"Average F1 score:\", average_f1_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# == Provide average scores ==\nprint('------------------------------------------------------------------------')\nprint('Score per fold')\nfor i in range(0, len(acc_per_fold)):\n  print('------------------------------------------------------------------------')\n  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\nprint('------------------------------------------------------------------------')\nprint('Average scores for all folds:')\nprint(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\nprint(f'> Loss: {np.mean(loss_per_fold)}')\nprint('------------------------------------------------------------------------')","metadata":{"scrolled":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport json\n\ndef plot_history(his):\n    \"\"\"\n    Plots the training and validation accuracy and loss of a Keras model.\n    \"\"\"\n    # Save the history to a JSON file\n    with open('history.json', 'w') as f:\n        json.dump(str(his.history), f)\n\n    # Create a DataFrame from the history object\n    history_df = pd.DataFrame(his.history)\n\n    # Plot the training and validation accuracy\n    plt.subplot(2, 1, 1)\n    plt.plot(history_df['accuracy'])\n    plt.plot(history_df['val_accuracy'])\n    plt.title('Model accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='lower right')\n    plt.ylim([0, 1])\n\n    # Plot the training and validation loss\n    plt.subplot(2, 1, 2)\n    plt.plot(history_df['loss'])\n    plt.plot(history_df['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='upper right')\n    plt.ylim([0, max(history_df['loss'])+1])\n\n    # Show the plot\n    plt.tight_layout()\n    plt.show()\n\nplot_history(history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_weights(\"VGG16_x5.weights.best.hdf5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_val_pred = model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_pred = model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tta_steps = 10\npredictions = []\nfor i in tqdm(range(tta_steps)):\n    preds = model.predict(X_test,\n                                    steps = 1)#len(X_test)/BATCH_SIZE)\n    \n    predictions.append(preds)\n    gc.collect()\n    \nY_pred_tta = np.mean(predictions, axis=0)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(Y_pred_tta)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#confusion matrix\nfrom sklearn.metrics import confusion_matrix\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=55)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n  \n\ncm = confusion_matrix(np.argmax(Y_test, axis=1), np.argmax(Y_pred, axis=1))\n\ncm_plot_label =['healthy', 'infected']\nplot_confusion_matrix(cm, cm_plot_label, title ='Confusion Metrix for ALL')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(np.argmax(Y_test, axis=1), np.argmax(Y_pred_tta, axis=1))\n\ncm_plot_label =['healty', 'infected']\nplot_confusion_matrix(cm, cm_plot_label, title ='Confusion Metrix for ALL')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report( np.mean(Y_test, axis=1), np.mean(Y_pred_tta, axis=1), digits=4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nclassification_report( np.argmax(Y_test, axis=1), np.argmax(Y_pred_tta, axis=1))\n\nfrom sklearn.metrics import roc_auc_score, auc\nfrom sklearn.metrics import roc_curve\nroc_log = roc_auc_score(np.argmax(Y_test, axis=1), np.argmax(Y_pred_tta, axis=1))\nfalse_positive_rate, true_positive_rate, threshold = roc_curve(np.argmax(Y_test, axis=1), np.argmax(Y_pred_tta, axis=1))\narea_under_curve = auc(false_positive_rate, true_positive_rate)\n\nplt.plot([0, 1], [0, 1], 'r--')\nplt.plot(false_positive_rate, true_positive_rate, label='AUC = {:.3f}'.format(area_under_curve))\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')\nplt.legend(loc='best')\nplt.show()\n#plt.savefig(ROC_PLOT_FILE, bbox_inches='tight')\nplt.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i=0\nprop_class=[]\nmis_class=[]\n\nfor i in range(len(Y_test)):\n        prop_class.append(i)\n    \n\ni=0\nfor i in range(len(Y_test)):\n        mis_class.append(i)\n   \n\n# # Display first 8 images of benign\nw=224\nh=224\nfig=plt.figure(figsize=(14, 30))\ncolumns = 5\nrows = 9\n\ndef Transfername(namecode):\n    if namecode==0:\n        return \"Healty\"\n    else:\n        return \"Infected\"\n    \nfor i in range(len(prop_class)):\n    ax = fig.add_subplot(rows, columns, i+1)\n    ax.set_title(\"Predicted result:\"+ Transfername(np.argmax(Y_pred_tta[prop_class[i]]))\n                       +\"\\n\"+\"Actual result: \"+ Transfername(np.argmax(Y_test[prop_class[i]])))\n                      \n    plt.imshow(X_test[prop_class[i]], interpolation='nearest')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}